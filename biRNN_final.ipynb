{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load old model\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read training data\n",
    "filename = \"data_train.txt\"\n",
    "\n",
    "try:\n",
    "    infile = open(filename, 'r')\n",
    "except IOError as error:\n",
    "    sys.stderr.write(\"File I/O error, reason\" + str(error) + \"\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "seqflag = False\n",
    "X_train = []\n",
    "Y_train = []\n",
    "seq_aa = []\n",
    "seq_ss = []\n",
    "for line in infile:\n",
    "    if line.startswith(\"end\") or line.startswith(\"<end>\"):\n",
    "        seqflag = False\n",
    "        X_train.append(seq_aa)\n",
    "        Y_train.append(seq_ss)\n",
    "        seq_aa = []\n",
    "        seq_ss = []\n",
    "    elif seqflag:\n",
    "        aa = line.split()[0]\n",
    "        ss = line.split()[1]\n",
    "        seq_aa.append(aa)\n",
    "        seq_ss.append(ss)\n",
    "    elif line.startswith(\"<>\"):\n",
    "        seqflag = True\n",
    "        \n",
    "# Read validation data\n",
    "filename = \"data_valid.txt\"\n",
    "\n",
    "try:\n",
    "    infile = open(filename, 'r')\n",
    "except IOError as error:\n",
    "    sys.stderr.write(\"File I/O error, reason\" + str(error) + \"\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "seqflag = False\n",
    "X_valid = []\n",
    "Y_valid = []\n",
    "seq_aa = []\n",
    "seq_ss = []\n",
    "for line in infile:\n",
    "    if line.startswith(\"end\") or line.startswith(\"<end>\"):\n",
    "        seqflag = False\n",
    "        X_valid.append(seq_aa)\n",
    "        Y_valid.append(seq_ss)\n",
    "        seq_aa = []\n",
    "        seq_ss = []\n",
    "    elif seqflag:\n",
    "        aa = line.split()[0]\n",
    "        ss = line.split()[1]\n",
    "        seq_aa.append(aa)\n",
    "        seq_ss.append(ss)\n",
    "    elif line.startswith(\"<>\"):\n",
    "        seqflag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define encoding dictionaries\n",
    "aadict = {'A':[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], \n",
    "          'R':[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'N':[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'D':[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'C':[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'Q':[0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'E':[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'G':[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'H':[0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'I':[0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "          'L':[0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
    "          'K':[0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "          'M':[0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "          'F':[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "          'P':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "          'S':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0],                                    \n",
    "          'T':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "          'W':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
    "          'Y':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "          'V':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    " }\n",
    "\n",
    "ssdict = {'_':[1,0,0], \n",
    "          'e':[0,1,0],\n",
    "          'h':[0,0,1],\n",
    " }\n",
    "\n",
    "# One-hot-encode training data\n",
    "for i in range(0,len(X_train)):\n",
    "    seq_aa = X_train[i]\n",
    "    seq_ss = Y_train[i]\n",
    "\n",
    "    for j in range(0,len(seq_ss)):\n",
    "        aa = seq_aa[j]\n",
    "        ss = seq_ss[j]\n",
    "        \n",
    "        if aa not in aadict:\n",
    "            print(\"Unknown aa: \" + aa)\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            X_train[i][j] = aadict[aa]\n",
    "        \n",
    "        if ss not in ssdict:\n",
    "            print(\"Unknown ss: \" + ss)\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            Y_train[i][j] = ssdict[ss]\n",
    "\n",
    "# One-hot-encode validation data\n",
    "for i in range(0,len(X_valid)):\n",
    "    seq_aa = X_valid[i]\n",
    "    seq_ss = Y_valid[i]\n",
    "\n",
    "    for j in range(0,len(seq_ss)):\n",
    "        aa = seq_aa[j]\n",
    "        ss = seq_ss[j]\n",
    "        \n",
    "        if aa not in aadict:\n",
    "            print(\"Unknown aa: \" + aa)\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            X_valid[i][j] = aadict[aa]\n",
    "        \n",
    "        if ss not in ssdict:\n",
    "            print(\"Unknown ss: \" + ss)\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            Y_valid[i][j] = ssdict[ss]\n",
    "\n",
    "# Get sequence lengths\n",
    "X_train_lengths = [0]*len(X_train)\n",
    "X_valid_lengths = [0]*len(X_valid)\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train_lengths[i] = len(X_train[i])\n",
    "\n",
    "for i in range(len(X_valid)):\n",
    "    X_valid_lengths[i] = len(X_valid[i])            \n",
    "            \n",
    "# Do padding to same sequence length\n",
    "numseqs_train = len(X_train)\n",
    "numseqs_valid = len(X_valid)\n",
    "maxseqlen = max(len(max(X_train,key=len)),len(max(X_valid,key=len)))\n",
    "\n",
    "X_train = pad_sequences(X_train,padding=\"post\",maxlen=maxseqlen)\n",
    "Y_train = pad_sequences(Y_train,padding=\"post\",maxlen=maxseqlen)\n",
    "\n",
    "X_valid = pad_sequences(X_valid,padding=\"post\",maxlen=maxseqlen)\n",
    "Y_valid = pad_sequences(Y_valid,padding=\"post\",maxlen=maxseqlen) \n",
    "\n",
    "# Reshape Ys to correct dimensions\n",
    "Y_train = np.reshape(Y_train,[-1,3])\n",
    "Y_valid = np.reshape(Y_valid,[-1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Build the network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Define placeholders\n",
    "num_classes = 3\n",
    "\n",
    "X_ph = tf.placeholder(tf.float32, [None,maxseqlen,20], name='XPlaceholder')\n",
    "X_len_ph = tf.placeholder(tf.int32, [None], name='XlenPlaceholder')\n",
    "Y_ph = tf.placeholder(tf.float32, [None,num_classes], name='YPlaceholder')\n",
    "phase_ph = tf.placeholder(tf.bool,name='phasePlaceholder')\n",
    "keep_prob_ph = tf.placeholder(tf.float32,name='keepprobPlaceholder')\n",
    "\n",
    "## Define the model\n",
    "\n",
    "# Initialize weights\n",
    "weight_initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "\n",
    "# Create biRNN layer\n",
    "RNN_units = 100\n",
    "\n",
    "with tf.variable_scope('layer1'):    \n",
    "    cell_fw = tf.nn.rnn_cell.LSTMCell(RNN_units)\n",
    "    cell_bw = tf.nn.rnn_cell.LSTMCell(RNN_units)    \n",
    "    \n",
    "    with tf.variable_scope('layer1_output'):\n",
    "        l_1, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw, cell_bw=cell_bw, inputs=X_ph, \n",
    "                                             sequence_length=X_len_ph, dtype=tf.float32)\n",
    "        l_1 = tf.concat(l_1,2) # Merge forward/backward LSTMs\n",
    "        l_1_reshaped = tf.reshape(l_1,[-1,2*RNN_units])\n",
    "        l_1_reshaped = tf.contrib.layers.batch_norm(l_1_reshaped, center=True, scale=True, is_training=phase_ph)\n",
    "\n",
    "# Create fully-connected/drop-out layer\n",
    "FFNN_units = 50\n",
    "\n",
    "with tf.variable_scope('layer2'): \n",
    "    W_2 = tf.get_variable('W_2', [2*RNN_units,FFNN_units], \n",
    "                          initializer=weight_initializer)\n",
    "    b_2 = tf.get_variable('b_2', [FFNN_units],\n",
    "                          initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    with tf.variable_scope('layer3_output'): \n",
    "            l_2 = tf.matmul(l_1_reshaped, W_2) + b_2\n",
    "            l_2 = tf.maximum(l_2, 0.01*l_2) # Leaky relu\n",
    "            l_2 = tf.nn.dropout(l_2, keep_prob = keep_prob_ph)\n",
    "            l_2 = tf.contrib.layers.batch_norm(l_2, center=True, scale=True, is_training=phase_ph)\n",
    "\n",
    "# Create softmax layer\n",
    "with tf.variable_scope('layer3'): \n",
    "    W_3 = tf.get_variable('W_3', [FFNN_units, num_classes], \n",
    "                          initializer=weight_initializer)\n",
    "    b_3 = tf.get_variable('b_3', [num_classes],\n",
    "                          initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    with tf.variable_scope('layer3_output'):\n",
    "        l_3 = tf.matmul(l_2, W_3) + b_3\n",
    "              \n",
    "Y = tf.nn.softmax(l_3)\n",
    "\n",
    "# Print number of trainable parameters\n",
    "print('Model consits of ', utils.num_params(), 'trainable parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def pred(X_in, sess):\n",
    "    feed_dict = {X_ph: X_in}\n",
    "    fetches = [Y]\n",
    "    res = sess.run(fetches, feed_dict)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Implement training ops\n",
    "\n",
    "# Define the cross entropy loss\n",
    "with tf.variable_scope('loss'):\n",
    "    \n",
    "    # Mask padding \n",
    "    boolmask = tf.equal(tf.reduce_sum(Y_ph,axis=1), 1)\n",
    "    Y_ph_masked = tf.boolean_mask(Y_ph,boolmask)\n",
    "    Y_masked = tf.boolean_mask(Y,boolmask)\n",
    "    \n",
    "    # Compute loss  \n",
    "    cross_entropy = -tf.reduce_sum(Y_ph_masked * tf.log(Y_masked), reduction_indices=[1]) \n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # L2 regularization\n",
    "    reg_scale = 0.0001\n",
    "    regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    reg_term = sum([regularize(param) for param in params])\n",
    "    cross_entropy += reg_term\n",
    "\n",
    "# Define the training op\n",
    "with tf.variable_scope('trainOP'):\n",
    "    \n",
    "    # Apply Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    # Clip gradients\n",
    "    gvs = optimizer.compute_gradients(cross_entropy)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "    train_op = optimizer.apply_gradients(capped_gvs)    \n",
    "\n",
    "# Define the accuracy op\n",
    "with tf.variable_scope('performance'):\n",
    "    \n",
    "    # Mask padding \n",
    "    boolmask = tf.equal(tf.reduce_sum(Y_ph,axis=1), 1)\n",
    "    Y_ph_masked = tf.boolean_mask(Y_ph,boolmask)\n",
    "    Y_masked = tf.boolean_mask(Y,boolmask)\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(Y_masked, axis=1), tf.argmax(Y_ph_masked, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Start the session\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts))\n",
    "\n",
    "if load_model:\n",
    "    try:\n",
    "        tf.train.Saver().restore(sess, \"/save/model.ckpt\")\n",
    "        print(\"Using saved model\")\n",
    "    except:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Model not found, new parameters initialized')\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "max_epochs = 20000\n",
    "dropout_prob_train = 0.5\n",
    "dropout_prob_val = 1.0\n",
    "\n",
    "train_cost, val_cost, train_acc, val_acc = [],[],[],[]\n",
    "\n",
    "try:       \n",
    "    for e in range(max_epochs):\n",
    "            \n",
    "        ## Fetch random batches\n",
    "        batchsize_train = 1 # Number of sequences\n",
    "        batchsize_valid = 1\n",
    "        \n",
    "        # Get random indices\n",
    "        idx_train = np.random.choice(range(len(X_train)),batchsize_train,replace=False)\n",
    "        idx_valid = np.random.choice(range(len(X_valid)),batchsize_valid,replace=False)\n",
    "\n",
    "        # Get X batches\n",
    "        X_train_batch = X_train[idx_train,:]\n",
    "        X_valid_batch = X_valid[idx_valid,:]\n",
    "        \n",
    "        # Get sequence lengths of X batches\n",
    "        X_train_lengths_batch = [X_train_lengths[idx_train[i]] for i in range(len(idx_train))]\n",
    "        X_valid_lengths_batch = [X_valid_lengths[idx_valid[i]] for i in range(len(idx_valid))]\n",
    "        \n",
    "        # Get Y batches \n",
    "        idx_train_rollout = [maxseqlen*idx_train[i] for i in range(len(idx_train))]\n",
    "        idx_valid_rollout = [maxseqlen*idx_valid[i] for i in range(len(idx_valid))]\n",
    "        \n",
    "        Y_train_batch = [Y_train[idx_train_rollout[i]:idx_train_rollout[i]+maxseqlen,:] for i in range(len(idx_train_rollout))]\n",
    "        Y_valid_batch = [Y_valid[idx_valid_rollout[i]:idx_valid_rollout[i]+maxseqlen,:] for i in range(len(idx_valid_rollout))]     \n",
    "        Y_train_batch = np.vstack(Y_train_batch)\n",
    "        Y_valid_batch = np.vstack(Y_valid_batch)\n",
    "        \n",
    "        # 1) Run the train op\n",
    "        feed_dict_train = {X_ph: X_train_batch, X_len_ph: X_train_lengths_batch, Y_ph: Y_train_batch, \n",
    "                           phase_ph: 1, keep_prob_ph: dropout_prob_train}\n",
    "        fetches_train = [train_op, cross_entropy, accuracy]\n",
    "        res = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "            \n",
    "        # 2) Compute train_cost, val_cost, train_acc, val_acc\n",
    "        train_cost += [res[1]]\n",
    "        train_acc += [res[2]]\n",
    "            \n",
    "        # 3) Run validation\n",
    "        feed_dict_valid = {X_ph: X_valid_batch, X_len_ph: X_valid_lengths_batch, Y_ph: Y_valid_batch, \n",
    "                           phase_ph: 0, keep_prob_ph: dropout_prob_val}\n",
    "        fetches_valid = [cross_entropy, accuracy]\n",
    "        res = sess.run(fetches=fetches_valid, feed_dict=feed_dict_valid)\n",
    "            \n",
    "        val_cost += [res[0]]\n",
    "        val_acc += [res[1]]\n",
    "            \n",
    "        # Print training summaries\n",
    "        if e % 100 == 0:\n",
    "            print(\"Epoch %i, Train Cost: %0.3f\\tVal Cost: %0.3f\\t Val acc: %0.3f\" \\\n",
    "                %(e, train_cost[-1],val_cost[-1],val_acc[-1]))\n",
    "                \n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define plot size\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "# 1) Plot train and validation loss as a function of epochs\n",
    "epoch = np.arange(len(train_cost))\n",
    "fig.add_subplot(121)\n",
    "plt.title('Loss')\n",
    "plt.plot(epoch, train_cost,'r', label='Train Loss')\n",
    "plt.plot(epoch, val_cost,'b', label='Val Loss')\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('Epochs'), plt.ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 2) Plot train and validation accuracy as a function of epochs\n",
    "fig.add_subplot(122)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(epoch, train_acc,'r', label='Train Accuracy')\n",
    "plt.plot(epoch, val_acc,'b', label='Val Accuracy')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('Epochs'), plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_path = tf.train.Saver().save(sess, \"/tmp/model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create confusion matrix from validation data results\n",
    "feed_dict_valid = {X_ph: X_valid, X_len_ph: X_valid_lengths, Y_ph: Y_valid, phase_ph: 0, keep_prob_ph: dropout_prob_val}\n",
    "fetches_preds = [Y]\n",
    "preds = sess.run(fetches=fetches_preds, feed_dict=feed_dict_valid)\n",
    "preds = np.vstack(preds)\n",
    "tmp = np.argmax(preds,axis=1).reshape(-1)\n",
    "preds = np.eye(3)[tmp]\n",
    "\n",
    "# Mask padding\n",
    "boolmask = np.equal(np.sum(Y_valid,axis=1), 1).tolist()\n",
    "Y_valid_masked = Y_valid[boolmask,:]\n",
    "preds_masked = preds[boolmask,:]\n",
    "\n",
    "# Compute metrics\n",
    "preds_masked_dense = np.argmax(preds_masked, axis=1)\n",
    "Y_valid_masked_dense = np.argmax(Y_valid_masked, axis=1)\n",
    "confusionmat = confusion_matrix(Y_valid_masked_dense,preds_masked_dense)\n",
    "print(confusionmat)\n",
    "print(\"The total validation accuracy is: \",(confusionmat[0,0]+confusionmat[1,1]+confusionmat[2,2])/np.sum(confusionmat))\n",
    "print(\"The random coil precision is: \",confusionmat[0,0]/np.sum(confusionmat[0,:]))\n",
    "print(\"The random coil recall is: \",confusionmat[0,0]/np.sum(confusionmat[:,0]))\n",
    "print(\"The beta sheet precision is: \",confusionmat[1,1]/np.sum(confusionmat[1,:]))\n",
    "print(\"The beta sheet recall is: \" ,confusionmat[1,1]/np.sum(confusionmat[:,1]))\n",
    "print(\"The alpha helix precision is: \",confusionmat[2,2]/np.sum(confusionmat[2,:]))\n",
    "print(\"The alpha helix recall is: \",confusionmat[2,2]/np.sum(confusionmat[:,2]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "552px",
    "left": "833px",
    "right": "20px",
    "top": "131px",
    "width": "560px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
