{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import utils\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load old model\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read training data\n",
    "filename = \"data_train.txt\"\n",
    "\n",
    "try:\n",
    "    infile = open(filename, 'r')\n",
    "except IOError as error:\n",
    "    sys.stderr.write(\"File I/O error, reason\" + str(error) + \"\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "seqflag = False\n",
    "X_train = []\n",
    "Y_train = []\n",
    "seq_aa = []\n",
    "seq_ss = []\n",
    "for line in infile:\n",
    "    if line.startswith(\"end\") or line.startswith(\"<end>\"):\n",
    "        seqflag = False\n",
    "        X_train.append(seq_aa)\n",
    "        Y_train.append(seq_ss)\n",
    "        seq_aa = []\n",
    "        seq_ss = []\n",
    "    elif seqflag:\n",
    "        aa = line.split()[0]\n",
    "        ss = line.split()[1]\n",
    "        seq_aa.append(aa)\n",
    "        seq_ss.append(ss)\n",
    "    elif line.startswith(\"<>\"):\n",
    "        seqflag = True\n",
    "        \n",
    "# Read validation data\n",
    "filename = \"data_valid.txt\"\n",
    "\n",
    "try:\n",
    "    infile = open(filename, 'r')\n",
    "except IOError as error:\n",
    "    sys.stderr.write(\"File I/O error, reason\" + str(error) + \"\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "seqflag = False\n",
    "X_valid = []\n",
    "Y_valid = []\n",
    "seq_aa = []\n",
    "seq_ss = []\n",
    "for line in infile:\n",
    "    if line.startswith(\"end\") or line.startswith(\"<end>\"):\n",
    "        seqflag = False\n",
    "        X_valid.append(seq_aa)\n",
    "        Y_valid.append(seq_ss)\n",
    "        seq_aa = []\n",
    "        seq_ss = []\n",
    "    elif seqflag:\n",
    "        aa = line.split()[0]\n",
    "        ss = line.split()[1]\n",
    "        seq_aa.append(aa)\n",
    "        seq_ss.append(ss)\n",
    "    elif line.startswith(\"<>\"):\n",
    "        seqflag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define encoding dictionaries\n",
    "aadict = {'A':[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], \n",
    "          'R':[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'N':[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'D':[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'C':[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'Q':[0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'E':[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'G':[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'H':[0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          'I':[0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "          'L':[0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
    "          'K':[0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "          'M':[0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "          'F':[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "          'P':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "          'S':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0],                                    \n",
    "          'T':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "          'W':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
    "          'Y':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "          'V':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    " }\n",
    "\n",
    "ssdict = {'_':[1,0,0], \n",
    "          'e':[0,1,0],\n",
    "          'h':[0,0,1],\n",
    " }\n",
    "           \n",
    "# One-hot-encode training data\n",
    "for i in range(0,len(X_train)):\n",
    "    seq_aa = X_train[i]\n",
    "    seq_ss = Y_train[i]\n",
    "\n",
    "    for j in range(0,len(seq_ss)):\n",
    "        aa = seq_aa[j]\n",
    "        ss = seq_ss[j]\n",
    "        \n",
    "        if aa not in aadict:\n",
    "            print(\"Unknown aa: \" + aa)\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            X_train[i][j] = aadict[aa]\n",
    "        \n",
    "        if ss not in ssdict:\n",
    "            print(\"Unknown ss: \" + ss)\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            Y_train[i][j] = ssdict[ss]\n",
    "\n",
    "# One-hot-encode validation data\n",
    "for i in range(0,len(X_valid)):\n",
    "    seq_aa = X_valid[i]\n",
    "    seq_ss = Y_valid[i]\n",
    "\n",
    "    for j in range(0,len(seq_ss)):\n",
    "        aa = seq_aa[j]\n",
    "        ss = seq_ss[j]\n",
    "        \n",
    "        if aa not in aadict:\n",
    "            print(\"Unknown aa: \" + aa)\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            X_valid[i][j] = aadict[aa]\n",
    "        \n",
    "        if ss not in ssdict:\n",
    "            print(\"Unknown ss: \" + ss)\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            Y_valid[i][j] = ssdict[ss]\n",
    "            \n",
    "# Do padding by adding half window size to each end\n",
    "windowsize = 17\n",
    "padding = int((windowsize-1)/2)\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = np.vstack((np.zeros((padding,20)),X_train[i],np.zeros((padding,20))))\n",
    "\n",
    "for i in range(len(X_valid)):\n",
    "    X_valid[i] = np.vstack((np.zeros((padding,20)),X_valid[i],np.zeros((padding,20))))\n",
    "\n",
    "# Collapse data structures to long array of  concatenated sequences\n",
    "X_train = np.vstack(X_train)\n",
    "Y_train = np.vstack(Y_train)\n",
    "\n",
    "X_valid = np.vstack(X_valid)\n",
    "Y_valid = np.vstack(Y_valid)\n",
    "\n",
    "# Window slicing\n",
    "X_train_windows = np.zeros([len(X_train)-windowsize+1,windowsize*20])\n",
    "X_valid_windows = np.zeros([len(X_valid)-windowsize+1,windowsize*20])\n",
    "\n",
    "for i in range(len(X_train_windows)):\n",
    "    X_train_windows[i,:] = np.reshape(X_train[i:i+windowsize,:],[1,windowsize*20])\n",
    "\n",
    "for i in range(len(X_valid_windows)):\n",
    "    X_valid_windows[i,:] = np.reshape(X_valid[i:i+windowsize,:],[1,windowsize*20])\n",
    "    \n",
    "# Masking\n",
    "boolmask_train = np.sum(X_train_windows[:,padding*20:padding*20+20],axis=1) > 0 #Check that central (predicted) aa is not zero\n",
    "boolmask_valid = np.sum(X_valid_windows[:,padding*20:padding*20+20],axis=1) > 0 \n",
    "\n",
    "X_train_windows_masked = X_train_windows[boolmask_train,:]\n",
    "X_valid_windows_masked = X_valid_windows[boolmask_valid,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model consits of  13763 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "## Build the network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Define placeholders\n",
    "features = windowsize*20\n",
    "num_classes = 3\n",
    "\n",
    "X_ph = tf.placeholder(tf.float32, [None,features], name='xPlaceholder')\n",
    "Y_ph = tf.placeholder(tf.float32, [None,num_classes], name='yPlaceholder')\n",
    "\n",
    "## Define the model\n",
    "\n",
    "# Initialize weights\n",
    "weight_initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "\n",
    "# Create hidden layer\n",
    "num_hiddennodes = 40\n",
    "\n",
    "with tf.variable_scope('layer1'): \n",
    "    W_1 = tf.get_variable('W_1', [features,num_hiddennodes], \n",
    "                          initializer=weight_initializer)\n",
    "    b_1 = tf.get_variable('b_1', [num_hiddennodes],\n",
    "                          initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    with tf.variable_scope('layer1_output'):\n",
    "        l_1 = tf.matmul(X_ph, W_1) + b_1\n",
    "        l_1 = tf.nn.relu(l_1)\n",
    "\n",
    "# Create softmax layer\n",
    "with tf.variable_scope('layer2'): \n",
    "    W_2 = tf.get_variable('W_2', [num_hiddennodes, num_classes], \n",
    "                          initializer=weight_initializer)\n",
    "    b_2 = tf.get_variable('b_2', [num_classes],\n",
    "                          initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    with tf.variable_scope('layer2_output'):\n",
    "         l_2 = tf.matmul(l_1, W_2) + b_2\n",
    "              \n",
    "Y = tf.nn.softmax(l_2)\n",
    "\n",
    "# Print number of trainable parameters\n",
    "print('Model consits of ', utils.num_params(), 'trainable parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def pred(X_in, sess):\n",
    "    feed_dict = {X_ph: X_in}\n",
    "    fetches = [Y]\n",
    "    res = sess.run(fetches, feed_dict)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Implement training ops\n",
    "\n",
    "# Define the cross entropy loss\n",
    "with tf.variable_scope('loss'):\n",
    "    \n",
    "    # Compute loss   \n",
    "    cross_entropy = -tf.reduce_sum(Y_ph * tf.log(Y), reduction_indices=[1]) \n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # L2 regularization\n",
    "    reg_scale = 0.01\n",
    "    regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    reg_term = sum([regularize(param) for param in params])\n",
    "    cross_entropy += reg_term\n",
    "\n",
    "# Define the training op\n",
    "with tf.variable_scope('trainOP'):\n",
    "    \n",
    "    # Apply Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    # Clip gradients\n",
    "    gvs = optimizer.compute_gradients(cross_entropy)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "    train_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "# Define the accuracy op\n",
    "with tf.variable_scope('performance'):\n",
    "       \n",
    "    # Compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, axis=1), tf.argmax(Y_ph, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Start the session\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts))\n",
    "\n",
    "if load_model:\n",
    "    try:\n",
    "        tf.train.Saver().restore(sess, \"/save/model.ckpt\")\n",
    "        print(\"Using saved model\")\n",
    "    except:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Model not found, new parameters initialized')\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "max_epochs = 20000\n",
    "\n",
    "train_cost, val_cost, train_acc, val_acc = [],[],[],[]\n",
    "\n",
    "try:       \n",
    "    for e in range(max_epochs):\n",
    "            \n",
    "        ## Fetch random batches\n",
    "        batchsize_train = 300 # Number of amino acids\n",
    "        batchsize_valid = 150\n",
    "        \n",
    "        # Get random indices \n",
    "        idx_train = np.random.choice(range(len(X_train_windows_masked)),batchsize_train,replace=False)\n",
    "        idx_valid = np.random.choice(range(len(X_valid_windows_masked)),batchsize_valid,replace=False)\n",
    "\n",
    "        # Get X batches\n",
    "        X_train_batch = X_train_windows_masked[idx_train,:]       \n",
    "        X_valid_batch = X_valid_windows_masked[idx_valid,:]\n",
    "        \n",
    "        # Get Y batches\n",
    "        Y_train_batch = Y_train[idx_train,:]\n",
    "        Y_valid_batch = Y_valid[idx_valid,:] \n",
    "                        \n",
    "        # 1) Run the train op\n",
    "        feed_dict_train = {X_ph: X_train_batch, Y_ph: Y_train_batch}\n",
    "        fetches_train = [train_op, cross_entropy, accuracy]\n",
    "        res = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "            \n",
    "        # 2) Compute train_cost, val_cost, train_acc, val_acc\n",
    "        train_cost += [res[1]]\n",
    "        train_acc += [res[2]]\n",
    "            \n",
    "        # 3) Run validation\n",
    "        feed_dict_valid = {X_ph: X_valid_batch, Y_ph: Y_valid_batch}\n",
    "        fetches_valid = [cross_entropy, accuracy]\n",
    "        res = sess.run(fetches=fetches_valid, feed_dict=feed_dict_valid)\n",
    "            \n",
    "        val_cost += [res[0]]\n",
    "        val_acc += [res[1]]\n",
    "            \n",
    "        # Print training summaries\n",
    "        if e % 1000 == 0:\n",
    "            print(\"Epoch %i, Train Cost: %0.3f\\tVal Cost: %0.3f\\t Val acc: %0.3f\" \\\n",
    "                %(e, train_cost[-1],val_cost[-1],val_acc[-1]))\n",
    "            \n",
    "            # Print intermediate predictions\n",
    "            fetches_print = [Y]\n",
    "            pred_intermediate = sess.run(fetches=fetches_print, feed_dict=feed_dict_train)\n",
    "            print(np.shape(pred_intermediate))\n",
    "            print(pred_intermediate)            \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define plot size\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "# 1) Plot train and validation loss as a function of epochs\n",
    "epoch = np.arange(len(train_cost))\n",
    "fig.add_subplot(121)\n",
    "plt.title('Loss')\n",
    "plt.plot(epoch, train_cost,'r', label='Train Loss')\n",
    "plt.plot(epoch, val_cost,'b', label='Val Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs'), plt.ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 2) Plot train and validation accuracy as a function of epochs\n",
    "fig.add_subplot(122)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(epoch, train_acc,'r', label='Train Accuracy')\n",
    "plt.plot(epoch, val_acc,'b', label='Val Accuracy')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('Epochs'), plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_path = tf.train.Saver().save(sess, \"/tmp/model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create confusion matrix from validation data results\n",
    "feed_dict_valid = {X_ph: X_valid_windows_masked, Y_ph: Y_valid}\n",
    "fetches = [Y]\n",
    "preds = sess.run(fetches=fetches, feed_dict=feed_dict_valid)\n",
    "preds = np.vstack(preds)\n",
    "tmp = np.argmax(preds,axis=1).reshape(-1)\n",
    "preds = np.eye(3)[tmp]\n",
    "\n",
    "# Mask padding\n",
    "boolmask = np.equal(np.sum(Y_valid,axis=1), 1).tolist()\n",
    "Y_valid_masked = Y_valid[boolmask,:]\n",
    "preds_masked = preds[boolmask,:]\n",
    "\n",
    "# Compute metrics\n",
    "preds_masked_dense = np.argmax(preds_masked, axis=1)\n",
    "Y_valid_masked_dense = np.argmax(Y_valid_masked, axis=1)\n",
    "confusionmat = confusion_matrix(Y_valid_masked_dense,preds_masked_dense)\n",
    "print(confusionmat)\n",
    "print(\"The total validation accuracy is: \",(confusionmat[0,0]+confusionmat[1,1]+confusionmat[2,2])/np.sum(confusionmat))\n",
    "print(\"The random coil precision is: \",confusionmat[0,0]/np.sum(confusionmat[0,:]))\n",
    "print(\"The random coil recall is: \",confusionmat[0,0]/np.sum(confusionmat[:,0]))\n",
    "print(\"The beta sheet precision is: \",confusionmat[1,1]/np.sum(confusionmat[1,:]))\n",
    "print(\"The beta sheet recall is: \" ,confusionmat[1,1]/np.sum(confusionmat[:,1]))\n",
    "print(\"The alpha helix precision is: \",confusionmat[2,2]/np.sum(confusionmat[2,:]))\n",
    "print(\"The alpha helix recall is: \",confusionmat[2,2]/np.sum(confusionmat[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot average magnitudes of weight groups\n",
    "weights = np.zeros(windowsize*20)\n",
    "weight_groups = np.empty(0)\n",
    "\n",
    "# Fetch weight values\n",
    "v = sess.run([v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"layer1\")])    \n",
    "\n",
    "# Average and separate\n",
    "for i in range(len(v[0])):\n",
    "    weights[i] = np.mean(v[0][i])\n",
    "\n",
    "for i in range(0,len(weights),20):\n",
    "    weight_groups = np.append(weight_groups,np.sum(np.abs(weights[i:i+20])))\n",
    "\n",
    "# Plot results\n",
    "fig = plt.figure()\n",
    "plt.plot(weight_groups, 'ro')\n",
    "plt.xticks(range(17), [-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8])\n",
    "plt.xlabel(\"Weight group position\")\n",
    "plt.ylabel(\"Average magnitude\")\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([min(weight_groups)-0.01,max(weight_groups)+0.01])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "552px",
    "left": "833px",
    "right": "20px",
    "top": "131px",
    "width": "560px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
